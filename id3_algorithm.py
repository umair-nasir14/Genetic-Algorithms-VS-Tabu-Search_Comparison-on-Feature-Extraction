# -*- coding: utf-8 -*-
"""id3_algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yJOjE_DE32TWGNSS0KlA9j9m1SpE_txk
"""

"""
Make the imports of python packages needed
"""
import pandas as pd
import numpy as np
from pprint import pprint
import math
from sklearn.feature_selection import chi2

'''

  DATASET LOADING AND PREPROCESSING

'''


dataset = pd.read_table('Training_Data.txt', delim_whitespace=True, names=('A', 'Target'))#Importing datasets and giving names
val_data = pd.read_table('Validation_Data.txt', delim_whitespace=True, names=('A', 'Target'))
test_data = pd.read_table('Test_Data.txt', delim_whitespace=True, names=('A', 'Target'))



dataset["A"] = dataset["A"].apply(list)#Seperating features through making a list

#Giving feature names
feature_names = []
for i in list(range(100)):
  feature_names.append('feature_'+ str(i))
dataset = pd.concat([dataset['A'].apply(pd.Series, index=feature_names), dataset["Target"]], axis=1)


#Similarly to Validation and test dataset
val_data["A"] = val_data["A"].apply(list)
val_data = pd.concat([val_data['A'].apply(pd.Series, index=feature_names), val_data["Target"]], axis=1)

test_data["A"] = test_data["A"].apply(list)
test_data = pd.concat([test_data['A'].apply(pd.Series, index=feature_names), test_data["Target"]], axis=1)



dataset.head()#Getting first five rows

'''
   
    CALCULATION OF ID3 DECISION TREE

'''


def entropy(target_col):
    """
    Calculate the entropy of a dataset.
    """
    elements,counts = np.unique(target_col,return_counts = True)
    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy


################### 
    
###################


def InfoGain(data,split_attribute_name,target_name="Target"):
    """
    Calculate the information gain of a dataset. This function takes three parameters:

    """    
    #Calculate the entropy of the total dataset
    total_entropy = entropy(data[target_name])
    
    ##Calculate the entropy of the dataset
    
    #Calculate the values and the corresponding counts for the split attribute 
    vals,counts= np.unique(data[split_attribute_name],return_counts=True)
    
    #Calculate the weighted entropy
    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])
    
    #Calculate the information gain
    Information_Gain = total_entropy - Weighted_Entropy
    return Information_Gain
       
###################

###################


def ID3(data,originaldata,features,target_attribute_name="Target",parent_node_class = None):
   
    #If all target_values have the same value, return this value
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]
    
    #If the dataset is empty, return the mode target feature value in the original dataset
    elif len(data)==0:
        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]
    
    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that
    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence
    #the mode target feature value is stored in the parent_node_class variable.
    
    elif len(features) ==0:
        return parent_node_class
    
    #If none of the above holds true, grow the tree!
    
    else:
        #Set the default value for this node --> The mode target feature value of the current node
        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]
        
        #Select the feature which best splits the dataset
        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        
        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information
        #gain in the first run
        tree = {best_feature:{}}
        
        
        #Remove the feature with the best inforamtion gain from the feature space
        features = [i for i in features if i != best_feature]
        
        #Grow a branch under the root node for each possible value of the root node feature
        
        for value in np.unique(data[best_feature]):
            value = value
            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets
            sub_data = data.where(data[best_feature] == value).dropna()
            
            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!
            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)
            
            #Add the sub tree, grown from the sub_dataset to the tree under the root node
            tree[best_feature][value] = subtree
            
        return(tree)    

print("***************************************************")
print("*          ORIGINAL ID3 DECISON TREE              *")
print("***************************************************")
tree = ID3(dataset,dataset,dataset.columns[:-1])
pprint(tree)

'''

   PREDICTING AND TESTING OUR ID3

'''

    
    
def predict(query,tree,default = 1):
   
    
    
    #1.
    for key in list(query.keys()):
        if key in list(tree.keys()):
            #2.
            try:
                result = tree[key][query[key]] 
            except:
                return default
  
            #3.
            result = tree[key][query[key]]
            #4.
            if isinstance(result,dict):
                return predict(query,result)

            else:
                return result

        
        


def test(data,tree):
    #Create new query instances by simply removing the target feature column from the original dataset and 
    #convert it to a dictionary
    queries = data.iloc[:,:-1].to_dict(orient = "records")
    
    #Create a empty DataFrame in whose columns the prediction of the tree are stored
    predicted = pd.DataFrame(columns=["Target"]) 
    
    #Calculate the prediction accuracy
    for i in range(len(data)):
        predicted.loc[i,"Target"] = predict(queries[i],tree,1.0) 

    #Getting True Positives, True Negatives, False Positives and False Negatives    
    TP = 0
    TN = 0
    FP = 0
    FN = 0

    for i in range(len(data)):
      if predicted.loc[i,"Target"] == data.loc[i,"Target"]:
        if predicted.loc[i,"Target"] == 1:
          TP += 1
        else:
          TN += 1
      if predicted.loc[i,"Target"] != data.loc[i,"Target"]:
        if predicted.loc[i,"Target"] == 1:
          FP += 1
        else:
          FN += 1
     

    accuracy = ((TP + TN)/(TP + TN + FP + FN))*100
    precision = (TP/(TP + FP))*100
    recall = (TP/(TP + FN))*100

    return accuracy,recall,precision,TP,TN,FP,FN

acc,rec,prec,tp,tn,fp,fn = test(test_data,tree)    

print("***************************************************")
print("*            ORIGINAL ID3 Performance             *")
print("***************************************************")


print(" accuracy = {}\n recall = {}\n precision = = {}\n TP = {}\n TN = = {}\n FP = {}\n FN = {}\n ".format(acc,rec,prec,tp,tn,fp,fn))

#Converting feature subset to 1s and 0s for chi square test
def convertData(data):
    for i in list(range(0,100)):
        data['feature_'+str(i)] = pd.Series(np.where(data['feature_'+str(i)].values == 'A', 1, 0),data.index)
    data['Target'] = pd.Series(np.where(data['Target'].values == bool('True'), 1, 0), data.index)
    return data
convertedData = convertData(dataset)

#Chi square test  
def chiSquareTest(data):
    X = data.drop('Target',axis=1)
    y = data['Target']
    chi_scores = chi2(X,y)
    chiSquareVal = chi_scores[0]
    pVal = chi_scores[1]
    return chiSquareVal, pVal